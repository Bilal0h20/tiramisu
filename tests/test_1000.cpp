#include "wrapper_test_1000.h"
#include <tiramisu/debug.h>
#include <tiramisu/core.h>
#include <Halide.h>

using namespace tiramisu;





// For computation C(i, k) = A(i, j) * B(j, k). Note that this code doesn't
// actually perform sparse matrix-matrix multiplication (SpMM); it could using
// an Object API, but the point of this test code is more to show the kinds of
// communication patterns that take place when writing this kind of code. So A,
// B, and C are dense matrices here, but the points are distributed across the
// ranks. (Using an Object API, these points would correspond to opaque objects
// such as sparse tensor blocks, and user-defined functions would operate on
// these objects via o_call.)
//
// In this test, we start with the points of A(i, j) and B(j, k) distributed
// among ranks, and those points must be moved to different ranks to compute the
// points of C(i, k) using owner's compute. Conceptually, the computation of C
// is distributed over i and k, while j can be thought of as a variable which
// represents different rounds of computation. (E.g. so that during round j = 0,
// The rank owning C(i, k) multiplies the points A(i, 0) and B(0, k) and sums
// the result into C(i, k). MPI sends and recvs must be used to move the points
// of A and B there prior to that computation.)
//
// The points are distributed in a round-robin fashion among the ranks. If we
// let the dimensions of the matrices A, B, and C be (I, J), (J, K), and (I, K),
// then we can define how the points of these matrices are distributed using
// using mapping key functions. More specifically, a mapping key function takes
// as input a point (e.g. (i, j)) and as output generates an integer, which is
// taken modulo the number of ranks to determine which rank the point is mapped
// to. We use simple mapping functions here, similar to the linearized addresses
// of Tiramisu. That is, for matrices A, B, and C, the mapping key functions are
// i*J+j, j*K+k, and i*K+k respectively. The mapping key size is just one more
// than the largest value of a mapping key function. Then for matrices A, B, and
// C, the mapping key sizes are I*J, J*K, and I*K.
//
// So in this code, a point (i, j) of matrix A is originally located on rank
// (i*J+j)%R, where R is the number of ranks. Note that this means its possible
// for a rank to contain multiple points from a matrix; for a given rank r, we
// call these multiple points "tasks". (E.g. for a (10, 10) matrix with 15
// ranks, for rank 0, task 0 would be (0, 0), task 1 would be (1, 5), task 2
// would be (3, 0), ..., task 6 would be (9, 0)). Each rank may have a different
// number of tasks assigned to it, but generally we say the "number of tasks" is
// the maximum number of tasks assigned to any given rank. With these simple
// mapping key functions, the task ID itself for a given mapping key is just
//     task = floor(mapping_key / R)
// This means the mapping key for a point obeys the relation
//     mapping_key = task * R + rank
// The number of tasks in this context is just
//     num_tasks = 1 + floor((mapping_key_size - 1) / R)
// The points for a matrix are thus stored in each rank using a vector of length
// num_tasks. Collectively, these task vectors contain all points of the matrix.
//
// During each round j of computing the points of C, the point C(i, k) will need
// the points A(i, j) and B(j, k) to be on its rank, i.e. (i*K+k)%R. This means
// that for each point C(i, k), the rank (i*J+j)%R must execute an MPI send to
// send point A(i, j) to rank (i*K+k)%R, while rank (i*K+k)%R must execute an
// MPI recv to receive the A(i, j) from rank (i*J+j)%R. (This is conceptually a
// broadcast operation, but Tiramisu doesn't support that yet.) For data, the
// MPI send for C(i, k) will use the storage for point A(i, j), which
// corresponds to task (i*J+j)/R in its task vector for A. The MPI recv for
// C(i, k) will store its copy of A(i, j) in a task vector similar to the task
// vector for C; that is, the task vector length is the same as C's, and the
// task ID in this vector is (i*K+k)/R, the same as point (i, k) for C. (We
// could potentially perform an optimization where if the same point A(i, j) is
// sent to the same rank, we only send it once and don't duplicate its storage,
// but this makes the code more complex, and will probably be rolled into a
// more general shared memory optimization we'll implement later.) We do a
// similar process for matrix B.
//
// One optimization in this code is we use non-blocking, non-synchronous MPI
// calls (i.e., MPI_Isend and MPI_Irecv). This means we need to keep track of
// MPI_Request objects generated by the non-blocking MPI calls, and call
// MPI_Wait on those requests before the computation using the data being
// transferred by those MPI requests. Another optimization that's performed
// here is to shift back the MPI_Isend and MPI_Irecv operations in the loop
// level corresponding to j/rounds. This allows us to overlap communication and
// computation (provided either HCA offloading or MPI background threads are
// working properly). This loop shifting requires us to make more storage for
// the copies of A and B that get sent, which multiples in size by one more than
// the loop shift number.
void gen(std::string name, int num_ranks, uint64_t i_dim_size, uint64_t j_dim_size, uint64_t k_dim_size, uint64_t comm_shift) {

  tiramisu::init(name);
  function *fn0 = global::get_implicit_function();

  // -------------------------------------------------------
  // Layer I
  // -------------------------------------------------------

  // Create constants from function inputs.
  constant NUM_RANKS("NUM_RANKS", expr((int32_t) num_ranks));
  constant I("I", expr((int32_t) i_dim_size));
  constant J("J", expr((int32_t) j_dim_size));
  constant K("K", expr((int32_t) k_dim_size));
  constant COMM_SHIFT("COMM_SHIFT", expr((int32_t) comm_shift));

  // Create constants for mapping key sizes and max number of tasks.
  constant MAP_C_SIZE("MAP_C_SIZE", expr(expr(I) * expr(K)));
  constant MAP_A_SIZE("MAP_A_SIZE", expr(expr(I) * expr(J)));
  constant MAP_B_SIZE("MAP_B_SIZE", expr(expr(J) * expr(K)));
  constant NUM_TASKS_C("NUM_TASKS_C", expr(((MAP_C_SIZE - 1) / NUM_RANKS) + 1));
  constant NUM_TASKS_A("NUM_TASKS_A", expr(((MAP_A_SIZE - 1) / NUM_RANKS) + 1));
  constant NUM_TASKS_B("NUM_TASKS_B", expr(((MAP_B_SIZE - 1) / NUM_RANKS) + 1));

  // Create vars for distributing computation of C.
  var j("j", expr((int32_t) 0), expr(J));
  var map_c("map_c", expr((int32_t) 0), expr(MAP_C_SIZE)), rnk_c("rnk_c"), tsk_c("tsk_c");

  // Create vars for distributing original blocks of A and B.
  var map_a("map_a", expr((int32_t) 0), expr(MAP_A_SIZE)), rnk_a("rnk_a"), tsk_a("tsk_a");
  var map_b("map_b", expr((int32_t) 0), expr(MAP_B_SIZE)), rnk_b("rnk_b"), tsk_b("tsk_b");

  // Create computation for original input blocks (source memory of MPI send).
  input a_in("a_in", {map_a}, p_int64);
  input b_in("b_in", {map_b}, p_int64);

  // Create computation for local input blocks (destination memory of MPI recv).
  input a_local("a_local", {j, map_c}, p_int64);
  input b_local("b_local", {j, map_c}, p_int64);

  // Create computation to initialize output blocks.
  computation c_local_init("c_local_init", {map_c}, expr((int64_t) 0));

  // Create computation to sum into output blocks.
  computation c_local("c_local", {j, map_c}, p_int64);
  c_local.set_expression(c_local(j-1, map_c) + a_local(j, map_c) * b_local(j, map_c));

  // Create (non-blocking, non-synchronous) MPI send/recv for A blocks.
  constant ONE("ONE", expr((int32_t) 1));
  std::string map_a_str("[map_c/" + std::to_string(k_dim_size) + "]*" + std::to_string(j_dim_size) + "+j");
  std::string rnk_a_str("(" + map_a_str + ")%" + std::to_string(num_ranks));
  std::string rnk_a_plus_one_str("(" + rnk_a_str + ")+ONE");
  // [J,K,NUM_RANKS,MAP_C_SIZE]->{a_send[j,map_c,rnk_a]: 0<=j<J and 0<=map_c<MAP_C_SIZE and ([map_c/K]*J+j)%NUM_RANKS<=rnk_a<(([map_c/K]*J+j)%NUM_RANKS)+1}
  std::string a_iter_send("[J,K,NUM_RANKS,MAP_C_SIZE,ONE]->{a_send[j,map_c,rnk_a]: 0<=j<J and 0<=map_c<MAP_C_SIZE and " + rnk_a_str + "<=rnk_a<" + rnk_a_plus_one_str + "}");
  // [J,K,NUM_RANKS,MAP_C_SIZE]->{a_recv[j,map_c]: 0<=j<J and 0<=map_c<MAP_C_SIZE}
  std::string a_iter_recv("[J,K,NUM_RANKS,MAP_C_SIZE]->{a_recv[j,map_c]: 0<=j<J and 0<=map_c<MAP_C_SIZE}");
  xfer comm_a = computation::create_xfer(
      a_iter_send,
      a_iter_recv,
      expr(map_c % NUM_RANKS),
      expr(((map_c / K) * J + j) % NUM_RANKS),
      xfer_prop(p_int64, {MPI, NONBLOCK, ASYNC}),
      xfer_prop(p_int64, {MPI, NONBLOCK, ASYNC}),
      a_in((map_c / K) * J + j),
      fn0);

  // Create waits for MPI send/recv for A blocks.
  tiramisu::wait wait_a_send((*comm_a.s)(j,map_c,rnk_a), xfer_prop(p_wait_ptr, {MPI}), fn0);
  tiramisu::wait wait_a_recv((*comm_a.r)(j,map_c), xfer_prop(p_wait_ptr, {MPI}), fn0);

  // Create (non-blocking, non-synchronous) MPI send/recv for B blocks.
  std::string map_b_str("j*" + std::to_string(k_dim_size) + "+(map_c%" + std::to_string(k_dim_size) + ")");
  std::string rnk_b_str("(" + map_b_str + ")%" + std::to_string(num_ranks));
  std::string rnk_b_plus_one_str("(" + rnk_b_str + ")+ONE");
  // [J,K,NUM_RANKS,MAP_C_SIZE]->{b_send[j,map_c,rnk_b]: 0<=j<J and 0<=map_c<MAP_C_SIZE and (j*K+(map_c%K))%NUM_RANKS<=rnk_b<((j*K+(map_c%K))%NUM_RANKS)+1}
  std::string b_iter_send("[J,K,NUM_RANKS,MAP_C_SIZE,ONE]->{b_send[j,map_c,rnk_b]: 0<=j<J and 0<=map_c<MAP_C_SIZE and " + rnk_b_str + "<=rnk_b<" + rnk_b_plus_one_str + "}");
  // [J,K,NUM_RANKS,MAP_C_SIZE]->{b_recv[j,map_c]: 0<=j<J and 0<=map_c<MAP_C_SIZE}
  std::string b_iter_recv("[J,K,NUM_RANKS,MAP_C_SIZE]->{b_recv[j,map_c]: 0<=j<J and 0<=map_c<MAP_C_SIZE}");
  xfer comm_b = computation::create_xfer(
      b_iter_send,
      b_iter_recv,
      expr(map_c % NUM_RANKS),
      expr((j * K + (map_c % K)) % NUM_RANKS),
      xfer_prop(p_int64, {MPI, NONBLOCK, ASYNC}),
      xfer_prop(p_int64, {MPI, NONBLOCK, ASYNC}),
      b_in(j * K + (map_c % K)),
      fn0);

  // Create waits for MPI send/recv for B blocks.
  tiramisu::wait wait_b_send((*comm_b.s)(j,map_c,rnk_b), xfer_prop(p_wait_ptr, {MPI}), fn0);
  tiramisu::wait wait_b_recv((*comm_b.r)(j,map_c), xfer_prop(p_wait_ptr, {MPI}), fn0);



  // -------------------------------------------------------
  // Layer II
  // -------------------------------------------------------

  // Distribute computation of output blocks.
  a_local.split(map_c, num_ranks, tsk_c, rnk_c);
  b_local.split(map_c, num_ranks, tsk_c, rnk_c);
  c_local_init.split(map_c, num_ranks, tsk_c, rnk_c);
  c_local.split(map_c, num_ranks, tsk_c, rnk_c);

  a_local.tag_distribute_level(rnk_c);
  b_local.tag_distribute_level(rnk_c);
  c_local_init.tag_distribute_level(rnk_c);
  c_local.tag_distribute_level(rnk_c);
  
  // Distribute MPI sends/recvs.
  comm_a.r->split(map_c, num_ranks, tsk_c, rnk_c);
  comm_b.r->split(map_c, num_ranks, tsk_c, rnk_c);

  comm_a.s->tag_distribute_level(rnk_a);
  comm_a.r->tag_distribute_level(rnk_c);
  comm_b.s->tag_distribute_level(rnk_b);
  comm_b.r->tag_distribute_level(rnk_c);

  // Distribute MPI waits.
  wait_a_recv.split(map_c, num_ranks, tsk_c, rnk_c);
  wait_b_recv.split(map_c, num_ranks, tsk_c, rnk_c);

  wait_a_send.tag_distribute_level(rnk_a);
  wait_a_recv.tag_distribute_level(rnk_c);
  wait_b_send.tag_distribute_level(rnk_b);
  wait_b_recv.tag_distribute_level(rnk_c);

  // Order computations and communication
  c_local_init.before(*comm_a.r, computation::root);
  comm_a.r->before(*comm_b.r, rnk_c);
  comm_b.r->before(*comm_a.s, j);
  comm_a.s->before(*comm_b.s, j);
  comm_b.s->before(wait_a_recv, j);
  wait_a_recv.before(wait_b_recv, rnk_c);
  wait_b_recv.before(c_local, rnk_c);
  c_local.before(wait_a_send, j);
  wait_a_send.before(wait_b_send, j);

  // Shift the sends/recvs backward in j so they overlap with computation.
  comm_a.r->shift(j, (int32_t) -comm_shift);
  comm_b.r->shift(j, (int32_t) -comm_shift);
  comm_a.s->shift(j, (int32_t) -comm_shift);
  comm_b.s->shift(j, (int32_t) -comm_shift);



  // -------------------------------------------------------
  // Layer III
  // -------------------------------------------------------

  // Make buffers long enough to store each task separately.
  buffer buf_a_in("buf_a_in", {expr(NUM_TASKS_A)}, p_int64, a_input);
  buffer buf_b_in("buf_b_in", {expr(NUM_TASKS_B)}, p_int64, a_input);
  buffer buf_wait_a_send("buf_wait_a_send", {expr(COMM_SHIFT + 1), expr(K), expr(NUM_TASKS_A)}, p_wait_ptr, a_input);
  buffer buf_wait_a_recv("buf_wait_a_recv", {expr(COMM_SHIFT + 1), expr(NUM_TASKS_C)}, p_wait_ptr, a_input);
  buffer buf_wait_b_send("buf_wait_b_send", {expr(COMM_SHIFT + 1), expr(I), expr(NUM_TASKS_B)}, p_wait_ptr, a_input);
  buffer buf_wait_b_recv("buf_wait_b_recv", {expr(COMM_SHIFT + 1), expr(NUM_TASKS_C)}, p_wait_ptr, a_input);
  buffer buf_a_local("buf_a_local", {expr(COMM_SHIFT + 1), expr(NUM_TASKS_C)}, p_int64, a_input);
  buffer buf_b_local("buf_b_local", {expr(COMM_SHIFT + 1), expr(NUM_TASKS_C)}, p_int64, a_input);
  buffer buf_c_local("buf_c_local", {expr(NUM_TASKS_C)}, p_int64, a_output);

  // Map computations to buffers.
  a_in.store_in(&buf_a_in, {expr(map_a / num_ranks)});
  b_in.store_in(&buf_b_in, {expr(map_b / num_ranks)});
  comm_a.r->store_in(&buf_a_local, {expr(j % ((int32_t) comm_shift + 1)), expr(map_c / num_ranks)});
  comm_b.r->store_in(&buf_b_local, {expr(j % ((int32_t) comm_shift + 1)), expr(map_c / num_ranks)});
  //comm_a.s->set_wait_access("[J,K,NUM_RANKS,COMM_SHIFT]->{a_send[j,map_c,rnk_a]->buf_wait_a_send[j%(COMM_SHIFT+1), map_c%NUM_RANKS, [([map_c/K]*J+j)/NUM_RANKS]]}");
  //comm_a.r->set_wait_access("[NUM_RANKS,COMM_SHIFT]->{a_recv[j,map_c]->buf_wait_a_recv[j%(COMM_SHIFT+1), [map_c/NUM_RANKS]]}");
  //comm_b.s->set_wait_access("[K,NUM_RANKS,COMM_SHIFT]->{b_send[j,map_c,rnk_b]->buf_wait_b_send[j%(COMM_SHIFT+1), [map_c/NUM_RANKS], [(j*K+(map_c%K))/NUM_RANKS]]}");
  //comm_b.r->set_wait_access("[NUM_RANKS,COMM_SHIFT]->{b_recv[j,map_c]->buf_wait_b_recv[j%(COMM_SHIFT+1), [map_c/NUM_RANKS]]}");
  std::string tsk_a_str("[(" + map_a_str + ")/" + std::to_string(num_ranks) + "]");
  std::string tsk_b_str("[(" + map_b_str + ")/" + std::to_string(num_ranks) + "]");
  std::string tsk_c_str("[map_c/" + std::to_string(num_ranks) + "]");
  std::string i_str("[map_c/" + std::to_string(k_dim_size) + "]");
  std::string k_str("map_c%" + std::to_string(k_dim_size));
  std::string comm_shift_str("j%" + std::to_string(comm_shift + 1));
  comm_a.s->set_wait_access("{a_send[j,map_c,rnk_a]->buf_wait_a_send[(" + comm_shift_str + "), (" + k_str + "), (" + tsk_a_str + ")]}");
  comm_a.r->set_wait_access("{a_recv[j,map_c]->buf_wait_a_recv[(" + comm_shift_str + "), (" + tsk_c_str + ")]}");
  comm_b.s->set_wait_access("{b_send[j,map_c,rnk_b]->buf_wait_b_send[(" + comm_shift_str + "), (" + i_str + "), (" + tsk_b_str + ")]}");
  comm_b.r->set_wait_access("{b_recv[j,map_c]->buf_wait_b_recv[(" + comm_shift_str + "), (" + tsk_c_str + ")]}");
  a_local.store_in(&buf_a_local, {expr(j % ((int32_t) comm_shift + 1)), expr(map_c / num_ranks)});
  b_local.store_in(&buf_b_local, {expr(j % ((int32_t) comm_shift + 1)), expr(map_c / num_ranks)});
  c_local_init.store_in(&buf_c_local, {expr(map_c / num_ranks)});
  c_local.store_in(&buf_c_local, {expr(map_c / num_ranks)});

  tiramisu::codegen({&buf_a_in, &buf_b_in, &buf_wait_a_send, &buf_wait_a_recv, &buf_wait_b_send, &buf_wait_b_recv, &buf_a_local, &buf_b_local, &buf_c_local}, "build/generated_fct_test_" + std::string(TEST_NUMBER_STR) + ".o");
  fn0->dump_halide_stmt();
}





int main(int argc, char **argv)
{
  gen("spmm", _NUM_RANKS, _I_DIM_SIZE, _J_DIM_SIZE, _K_DIM_SIZE, _COMM_SHIFT);
  return 0;
}
